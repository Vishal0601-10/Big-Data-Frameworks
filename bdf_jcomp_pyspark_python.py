# -*- coding: utf-8 -*-
"""BDF_JCOMP_PYSPARK_PYTHON.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cdVD2bGAOgw7g9JtvFEcC3QLEUjN4dze

**Pyspark Implementation**
"""

!apt-get update
# Download Java Virtual Machine (JVM)
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
# Download Spark
!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
# Unzip the file
!tar xf spark-3.5.1-bin-hadoop3.tgz
!pip install -q findspark

# set your spark folder to your system path environment.
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"

!ls

import findspark
findspark.init()
from pyspark import SparkContext
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # Property used to format output tables better
sc = spark.sparkContext
sc

import time
import matplotlib.pyplot as plt

# Track overall timings
times = {}

print("⏳ Tracking started...\n")

import time

# Start tracking
pipeline_start = time.time()
print("Timer started — tracking total execution time...")

# STEP 1: Setup

import zipfile, os
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType

spark = SparkSession.builder.appName("MergeYouTubeDataset").getOrCreate()


# STEP 2: Unzip the Kaggle dataset

zip_path = "/content/archive.zip"
extract_path = "/content/youtube_data"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)


# STEP 3: Read all CSVs and merge into one DataFrame

csv_files = [f for f in os.listdir(extract_path) if f.endswith('.csv')]
print("Found CSV files:", csv_files)

youtube_schema = StructType([
    StructField("video_id", StringType(), True),
    StructField("trending_date", StringType(), True),
    StructField("title", StringType(), True),
    StructField("channel_title", StringType(), True),
    StructField("category_id", StringType(), True),
    StructField("publish_time", StringType(), True),
    StructField("tags", StringType(), True),
    StructField("views", IntegerType(), True),
    StructField("likes", IntegerType(), True),
    StructField("dislikes", IntegerType(), True),
    StructField("comment_count", IntegerType(), True),
    StructField("thumbnail_link", StringType(), True),
    StructField("comments_disabled", BooleanType(), True),
    StructField("ratings_disabled", BooleanType(), True),
    StructField("video_error_or_removed", BooleanType(), True),
    StructField("description", StringType(), True)
])


all_dfs = []
for f in csv_files:
    country_code = f[:2]
    df = spark.read.csv(os.path.join(extract_path, f), header=True, schema=youtube_schema)
    df = df.withColumn("country", lit(country_code))
    all_dfs.append(df)

merged_df = all_dfs[0]
for df in all_dfs[1:]:
    merged_df = merged_df.unionByName(df, allowMissingColumns=True)

print("Combined dataset row count:", merged_df.count())


# STEP 4: Save merged dataset

output_path = "/content/Global_YouTube_Trends.csv"
merged_df.coalesce(1).write.csv(output_path, header=True, mode="overwrite")

print(f"Merged dataset saved to: {output_path}")

#YouTube View Forecasting using LSTM (Deep Learning Model)


import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_date, sum as _sum
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential # Import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler


#Create Spark Session
spark = SparkSession.builder.appName("YouTube_LSTM_Forecasting").getOrCreate()

# Load merged dataset
df = spark.read.csv("/content/Global_YouTube_Trends.csv", header=True, inferSchema=True)

# Aggregate daily total views
daily_df = (
    df.withColumn("publish_date", to_date("publish_time"))
      .groupBy("publish_date")
      .agg(_sum("views").alias("total_views"))
      .orderBy("publish_date")
      .toPandas()
)


#Preprocess for LSTM

data = daily_df[['total_views']].values
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(data)

# Create sequences (e.g., 60 days → next day)
def create_sequences(dataset, window_size=60):
    X, y = [], []
    for i in range(window_size, len(dataset)):
        X.append(dataset[i-window_size:i, 0])
        y.append(dataset[i, 0])
    return np.array(X), np.array(y)

window_size = 60
X, y = create_sequences(scaled_data, window_size)
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# Split train/test
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]


#Build LSTM Model

model = Sequential([
    LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], 1)),
    LSTM(50, return_sequences=False),
    Dense(25, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()


#Train Model

history = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_split=0.1, verbose=1)


#Evaluate + Predict
predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions.reshape(-1, 1))

actual = scaler.inverse_transform(y_test.reshape(-1, 1))

plt.figure(figsize=(12,5))
plt.plot(actual, label='Actual Views', color='blue')
plt.plot(predictions, label='Predicted Views', color='orange')
plt.title("YouTube Daily Views Forecast (LSTM Model)")
plt.xlabel("Days")
plt.ylabel("Total Views")
plt.legend()
plt.show()

from pyspark.sql.functions import to_date, sum as _sum
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Aggregate daily views again
daily_df = (
    df.withColumn("publish_date", to_date("publish_time"))
      .groupBy("publish_date")
      .agg(_sum("views").alias("total_views"))
      .orderBy("publish_date")
      .toPandas()
)

# Fill missing dates and smooth
daily_df['publish_date'] = pd.to_datetime(daily_df['publish_date'])
daily_df = daily_df.set_index('publish_date').asfreq('D').fillna(method='ffill')
daily_df['total_views'] = daily_df['total_views'].rolling(window=7, min_periods=1).mean()

scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(daily_df[['total_views']])

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

window_size = 30

X, y = [], []
for i in range(window_size, len(scaled_data)):
    X.append(scaled_data[i-window_size:i, 0])
    y.append(scaled_data[i, 0])

X, y = np.array(X), np.array(y)
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)),
    LSTM(50),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')
model.fit(X, y, epochs=30, batch_size=16, verbose=0)

last_seq = scaled_data[-window_size:].reshape(1, window_size, 1)
future_scaled = []

for _ in range(30):
    pred = model.predict(last_seq, verbose=0)
    future_scaled.append(pred[0, 0])
    # Reshape pred to match the dimensions of last_seq[:, 1:, :] before appending
    last_seq = np.append(last_seq[:, 1:, :], pred.reshape(1, 1, 1), axis=1)


future_values = scaler.inverse_transform(np.array(future_scaled).reshape(-1, 1))
future_dates = pd.date_range(daily_df.index[-1], periods=31, freq='D')[1:]

import matplotlib.pyplot as plt

plt.figure(figsize=(12,6))
plt.plot(daily_df.index[-60:], daily_df['total_views'].iloc[-60:], label='Last 60 Days Actual', color='blue')
plt.plot(future_dates, future_values, label='Next 30 Days Forecast', color='green', marker='o')
plt.title("YouTube Total Views Forecast (Next 30 Days - LSTM)")
plt.xlabel("Date")
plt.ylabel("Predicted Views")
plt.legend()
plt.grid(True)
plt.show()

# --- PART 1: LOAD ALL FILES FROM THE ZIP ---

from pyspark.sql import SparkSession
import zipfile, os, shutil
from pyspark.sql.functions import col, lit

spark = SparkSession.builder.appName("YouTubeTrendPredictor").getOrCreate()

zip_path = "/content/archive.zip"
extract_dir = "/content/youtube_data"

# extract CSVs
if os.path.exists(extract_dir):
    shutil.rmtree(extract_dir)
os.makedirs(extract_dir, exist_ok=True)
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# read all CSVs and add country column
csv_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')]
all_dfs = []
for f in csv_files:
    country_code = f[:2]  # e.g., "IN" from "INvideos.csv"
    df = spark.read.option("header", True).csv(os.path.join(extract_dir, f))
    df = df.withColumn("country", lit(country_code))
    all_dfs.append(df)

# Merge all dataframes
if all_dfs:
    df = all_dfs[0]
    for next_df in all_dfs[1:]:
        df = df.unionByName(next_df, allowMissingColumns=True)
else:
    df = spark.createDataFrame([], schema=None) # Create an empty DataFrame if no files are found


# basic cleanup
num_df = df.select(
    col("title"), col("description"), col("category_id").cast("int"),
    col("views").cast("int"), col("likes").cast("int"),
    col("comment_count").cast("int"), col("country")
).na.drop(subset=["title", "description", "views", "likes"])
num_df.show(3)

# --- PART 2: CREATE LABEL FOR TRENDING VIDEOS ---

from pyspark.sql.functions import when, percentile_approx

threshold = num_df.approxQuantile("views", [0.9], 0.0)[0]
labeled_df = num_df.withColumn("is_trending",
                when(col("views") >= threshold, 1).otherwise(0))
labeled_df.select("views", "is_trending").show(5)

# --- PART 3: TEXT + CATEGORY FEATURES ---

from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler
from pyspark.ml import Pipeline

tokenizer = Tokenizer(inputCol="description", outputCol="words")
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
tf = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=2000)
idf = IDF(inputCol="rawFeatures", outputCol="textFeatures")
cat_indexer = StringIndexer(inputCol="category_id", outputCol="categoryIndex", handleInvalid="keep")

assembler = VectorAssembler(
    inputCols=["textFeatures", "categoryIndex"],
    outputCol="features"
)

pipeline = Pipeline(stages=[tokenizer, remover, tf, idf, cat_indexer, assembler])
processed_df = pipeline.fit(labeled_df).transform(labeled_df)

# --- PART 4: TRAIN CLASSIFIER ---

from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=42)

rf = RandomForestClassifier(labelCol="is_trending", featuresCol="features", numTrees=30)
model = rf.fit(train_df)
preds = model.transform(test_df)

evaluator = BinaryClassificationEvaluator(labelCol="is_trending")
print("AUC:", evaluator.evaluate(preds))

# --- PART 5: PREDICT TREND PROBABILITY FOR NEW VIDEO ---

from pyspark.sql import Row
sample = spark.createDataFrame([
    Row(title="Funny fails compilation 2025",
        description="Best funny fails of the year 2025 – hilarious moments",
        category_id=24, country="US")
])

sample_proc = pipeline.fit(labeled_df).transform(sample)
result = model.transform(sample_proc)
result.select("title", "probability", "prediction").show(truncate=False)

# --- PART 6 (Enhanced): COUNTRY-AWARE TREND INSIGHT GENERATOR (USER INPUT VERSION) ---

from textblob import TextBlob
from pyspark.sql import Row
from pyspark.sql.functions import hour, date_format
import random

def predict_trend_countrywise(title, description, category_id, country):
    """
    Predict trend probability and give upload insights for a specific country.
    """

    # --- 1. Filter dataset for that country (for data-driven insights) ---
    country_df = labeled_df.filter(labeled_df.country == country)
    if country_df.count() < 50:
        # fallback if not enough data
        country_df = labeled_df
        print(f"Not enough data for {country}, using global stats.\n")

    # --- 2. Predict trending probability ---
    new_video = spark.createDataFrame([
        Row(title=title, description=description, category_id=str(category_id), country=country)
    ])
    new_proc = pipeline.fit(labeled_df).transform(new_video)
    result = model.transform(new_proc).collect()[0]
    trend_prob = float(result["probability"][1]) * 100

    # --- 3. Title sentiment analysis ---
    sentiment = TextBlob(title).sentiment.polarity
    if sentiment > 0.4:
        sentiment_msg = "Highly positive titles perform 1.5× better"
    elif sentiment > 0.1:
        sentiment_msg = "Mildly positive titles perform 1.3× better"
    elif sentiment < -0.1:
        sentiment_msg = "Negative titles perform 0.8× worse"
    else:
        sentiment_msg = "Neutral titles perform normally"

    # --- 4. Derive data-driven upload time & day (if timestamps exist) ---
    if "publish_time" in country_df.columns:
        from pyspark.sql.functions import hour, date_format

        time_df = (
            country_df
            .withColumn("hour", hour("publish_time"))
            .withColumn("day", date_format("publish_time", "EEEE"))
            .groupBy("hour", "day")
            .count()
            .orderBy("count", ascending=False)
        )

        best_times = time_df.limit(5).toPandas()
        if not best_times.empty:
            top_hour = int(best_times.iloc[0]['hour'])
            best_day = best_times.iloc[0]['day']
            upload_window = f"{top_hour:02d}:00–{(top_hour+2)%24:02d}:00 local time"
        else:
            upload_window, best_day = "6 PM–8 PM", random.choice(["Friday", "Saturday"])
    else:
        upload_window = "6 PM–8 PM local time"
        best_day = random.choice(["Friday", "Saturday", "Sunday"])

    # --- 5. Display the insights ---
    print(f"""
Input Video
──────────────────────────────────────
Title: "{title}"
Category: {category_id}
Target Country: {country}

Insights
──────────────────────────────────────
→ Best upload window: {upload_window}
→ Suggested posting day: {best_day}
→ Title sentiment insight: {sentiment_msg}
→ Predicted trend probability: {trend_prob:.2f} %
""")

# --- INTERACTIVE USER INPUT SECTION ---

print("Enter your YouTube video details for country-based trend analysis:\n")

user_title = input("Enter video title: ").strip()
user_description = input("Enter video description: ").strip()
user_category = input("Enter video category (e.g., Entertainment, Music, Sports): ").strip()
user_country = input("Enter target country code (e.g., IN, US, GB, JP): ").upper().strip()

# Run prediction with user-provided inputs
predict_trend_countrywise(
    title=user_title,
    description=user_description,
    category_id=user_category,
    country=user_country
)

Pipeline_end = time.time()
times["PySpark"] = Pipeline_end - pipeline_start
print(f"PySpark processing took: {times['PySpark']:.2f} seconds\n")

"""---

**Python Implementation**
"""

import time
import matplotlib.pyplot as plt

# Track overall timings
times = {}

print("⏳ Tracking started...\n")

t3 = time.time()

import pandas as pd
import os
import zipfile

# Path of YouTube zip dataset
zip_path = "/content/archive.zip"
extract_dir = "/content/youtube_data"

if not os.path.exists(extract_dir):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

# Load all CSVs into one DataFrame
dfs = []
for file in os.listdir(extract_dir):
    if file.endswith(".csv"):
        country = file.replace(".csv", "").upper()[:2]  # derive country
        try:
            df = pd.read_csv(os.path.join(extract_dir, file), encoding='utf-8')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(os.path.join(extract_dir, file), encoding='latin1')
            except UnicodeDecodeError:
                 try:
                     df = pd.read_csv(os.path.join(extract_dir, file), encoding='cp1252')
                 except Exception as e:
                     print(f"Could not read file {file} with multiple encodings: {e}")
                     continue # Skip to the next file if all attempts fail
        df["country"] = country
        dfs.append(df)

data = pd.concat(dfs, ignore_index=True)
data = data.dropna(subset=["title", "description", "views", "likes"])
print("Combined dataset shape:", data.shape)
data.head()

# Define trending as top 10% by views per country
data["views"] = data["views"].astype(int)
trending_flags = []

for country, group in data.groupby("country"):
    threshold = group["views"].quantile(0.9)
    trending_flags.extend((group["views"] >= threshold).astype(int).tolist())

data["is_trending"] = trending_flags
print(data[["country", "views", "is_trending"]].sample(5))

import pandas as pd
import time
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# --- Data Preprocessing ---
data["description"] = data["description"].fillna("")
data["category_id"] = data["category_id"].astype(str)
data["country"] = data["country"].astype(str)

X = data[["description", "category_id", "country", "likes", "comment_count"]]
y = data["is_trending"]

# --- Split Train/Test Data ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# --- Column Transformer ---
preprocessor = ColumnTransformer([
    ("desc", HashingVectorizer(n_features=1000, stop_words="english"), "description"),
    ("cat", OneHotEncoder(handle_unknown="ignore"), ["category_id", "country"]),
    ("num", StandardScaler(), ["likes", "comment_count"])
])

# --- Fast Model (Logistic Regression) ---
model = Pipeline([
    ("features", preprocessor),
    ("clf", LogisticRegression(max_iter=300, n_jobs=-1))
])

# --- Train & Track Time ---
start_time = time.time()
model.fit(X_train, y_train)
train_time = time.time() - start_time
print(f"Model trained successfully in {train_time:.2f} seconds.\n")

# --- Evaluate Performance ---
y_pred = model.predict(X_test)
print("Model Evaluation:")
print("Accuracy:", round(accuracy_score(y_test, y_pred), 3))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

from sklearn.metrics import classification_report, roc_auc_score

preds = model.predict(X_test)
probs = model.predict_proba(X_test)[:, 1]
print(classification_report(y_test, preds))
print("ROC-AUC:", roc_auc_score(y_test, probs))

data["publish_time"] = pd.to_datetime(data["publish_time"], errors='coerce')
data["hour"] = data["publish_time"].dt.hour
data["day"] = data["publish_time"].dt.day_name()

country_insights = (
    data.groupby(["country", "day", "hour"])["is_trending"]
    .mean()
    .reset_index()
    .sort_values(["country", "is_trending"], ascending=[True, False])
)

print(country_insights.head(10))

import seaborn as sns
import matplotlib.pyplot as plt

top_country = "US"
sns.heatmap(
    country_insights[country_insights["country"] == top_country]
    .pivot_table(index="day", columns="hour", values="is_trending"),
    cmap="YlGnBu"
)
plt.title(f"Trending Probability Heatmap – {top_country}")
plt.show()

from textblob import TextBlob
import pandas as pd
import random

def predict_trend_py(title, description, category_id, country):

    # Create input dataframe for model
    x_input = pd.DataFrame([{
        "description": description,
        "category_id": str(category_id),
        "country": country,
        "likes": 0,
        "comment_count": 0
    }])

    # --- Predict probability and class ---
    prob = model.predict_proba(x_input)[0][1]
    pred_class = "TRENDING" if prob >= 0.5 else "NOT TRENDING"

    # --- Sentiment analysis ---
    sentiment = TextBlob(title).sentiment.polarity
    if sentiment > 0.4:
        sentiment_msg = "Highly positive title — good engagement expected."
    elif sentiment > 0.1:
        sentiment_msg = "Mildly positive — likely to perform above average."
    elif sentiment < -0.1:
        sentiment_msg = "Negative tone — may underperform."
    else:
        sentiment_msg = "Neutral title — standard performance."

    # --- Best time/day from insights (safe version) ---
    best_hour, best_day = "18:00–20:00", random.choice(["Friday", "Saturday"])  # default fallback

    if "country" in country_insights.columns:
        sub = country_insights[country_insights["country"] == country]

        if not sub.empty and "is_trending" in sub.columns and sub["is_trending"].any():
            try:
                top_index = sub["is_trending"].idxmax()
                if pd.notna(top_index):
                    top_row = sub.loc[top_index]
                    if "hour" in top_row and "day" in top_row:
                        best_hour = f"{int(top_row['hour']):02d}:00–{(int(top_row['hour'])+2)%24:02d}:00"
                        best_day = top_row["day"]
            except Exception as e:
                print(f"Warning while fetching best time/day: {e}")

    # --- Display result ---
    print("=" * 60)
    print("YOUTUBE TREND PREDICTION (Python Version)")
    print("=" * 60)
    print(f"Title            : {title}")
    print(f"Description      : {description[:100]}...")
    print(f"Category         : {category_id}")
    print(f"Target Country   : {country}")
    print("-" * 60)
    print(f"Predicted Class       : {pred_class}")
    print(f"Trend Probability     : {prob * 100:.2f} %")
    print(f"Sentiment Insight     : {sentiment_msg}")
    print(f"Best Upload Window    : {best_hour}")
    print(f"Suggested Posting Day : {best_day}")
    print("=" * 60)


# --- USER INPUT SECTION ---
print("Enter your YouTube video details for trend prediction:\n")

title = input("Video Title: ").strip()
description = input("Video Description: ").strip()
category_id = input("Category (e.g., Entertainment, Music, Sports): ").strip()
country = input("Target Country Code (e.g., IN, US, GB, JP): ").upper().strip()

predict_trend_py(title, description, category_id, country)

t4 = time.time()
times["Python"] = t4 - t3
print(f"Python (Pandas) processing took: {times['Python']:.2f} seconds\n")

import matplotlib.pyplot as plt

if "PySpark" not in times:
    times["PySpark"] = 17.71
framework_order = ["PySpark", "Python"]

ordered_times = [times[framework] for framework in framework_order]
ordered_labels = framework_order

plt.figure(figsize=(6,4))
plt.bar(ordered_labels, ordered_times, color=['orange', 'skyblue'])
plt.title('Execution Time Comparison: PySpark vs Python')
plt.ylabel('Time (seconds)')
plt.xlabel('Framework')

for i, (label, value) in enumerate(zip(ordered_labels, ordered_times)):
    plt.text(label, value + 0.2, f"{value:.2f}s", ha='center', fontsize=10)

plt.show()

